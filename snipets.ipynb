{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA check\n",
    "We can crunch the numbers faster if we have access to a suitable GPU. If we have the GPU version of PyTorch and CUDA correctly configured, we can move the model and the data to the GPU. First we can check if the GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "CUDA capability: major: 8, minor: 6\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |   54784 B  |  254976 B  |  254976 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |   54784 B  |  254976 B  |  254976 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |   54784 B  |  254976 B  |  254976 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |   54784 B  |  254976 B  |  254976 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |    2047 KB |    4245 KB |    4245 KB |\n",
      "|       from large pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |       0 B  |    2047 KB |    4245 KB |    4245 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       9    |     296    |     296    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       9    |     296    |     296    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       9    |     296    |     296    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       9    |     296    |     296    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       4    |     127    |     127    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       4    |     127    |     127    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# checking GPU capability\n",
    "print(torch.cuda.get_device_name())\n",
    "print(\n",
    "    f\"CUDA capability: major: {torch.cuda.get_device_capability()[0]}, minor: {torch.cuda.get_device_capability()[1]}\"\n",
    ")\n",
    "print(torch.cuda.memory.memory_summary())\n",
    "\n",
    "# setting GPU as device if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
